# -*- coding: utf-8 -*-
"""website_scraping_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hpAIT7kAWeI6WksIlYPamTU3IOjEvSv4
"""

# load google drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from bs4 import BeautifulSoup
import requests
import os

structure_file_path = '/content/drive/MyDrive/20211030 Test Assignment-20230808T121525Z-001/20211030 Test Assignment/Output_Data_Structure.xlsx'
structure_df = pd.read_excel(structure_file_path)

from textblob import TextBlob
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import words
import pandas as pd

!pip install nltk

import nltk
nltk.download('punkt')

nltk.download('words')

import chardet

# Load the master dictionary files
master_dict_folder = '/content/drive/MyDrive/20211030 Test Assignment-20230808T121525Z-001/20211030 Test Assignment/MasterDictionary'
positive_words_path = os.path.join(master_dict_folder, 'positive-words.txt')
negative_words_path = os.path.join(master_dict_folder, 'negative-words.txt')

def detect_encoding(file_path):
    with open(file_path, 'rb') as file:
        result = chardet.detect(file.read())
        return result['encoding']

encoding = detect_encoding(negative_words_path)
print("Detected Encoding:", encoding)

def load_word_list(file_path, encoding='utf-8'):
  with open(file_path, 'r', encoding=encoding) as f:
    return set([word.strip() for word in f.readlines()])

positive_words = load_word_list(positive_words_path)
negative_words = load_word_list(negative_words_path, encoding = 'ISO-8859-1')

stop_words_directory = '/content/drive/MyDrive/20211030 Test Assignment-20230808T121525Z-001/20211030 Test Assignment/StopWords'

def load_all_stop_words(directory_path):
    all_stop_words = set()

    for filename in os.listdir(directory_path):
        if filename.endswith(".txt"):
            file_path = os.path.join(directory_path, filename)
            with open(file_path, 'r', encoding='latin-1') as file:
                stop_words_in_file = set([word.strip() for word in file.readlines()])
                all_stop_words.update(stop_words_in_file)

    return all_stop_words

stop_words = load_all_stop_words(stop_words_directory)

def perform_text_analysis(url, url_id, text):
    words = word_tokenize(text)
    # Remove stop words
    filtered_words = [word for word in words if word.lower() not in stop_words]
    # Join the filtered words back into a cleaned text
    cleaned_text = ' '.join(filtered_words)
    # Continue with the analysis using the cleaned_text
    blob = TextBlob(cleaned_text)

    # Load the positive and negative words from the master dictionary
    positive_words = load_word_list(positive_words_path)
    negative_words = load_word_list(negative_words_path, encoding='ISO-8859-1')

    # Remove stop words from the positive and negative words
    filtered_positive_words = [word for word in positive_words if word.lower() not in stop_words]
    filtered_negative_words = [word for word in negative_words if word.lower() not in stop_words]

    # Create dictionaries of positive and negative words
    positive_word_dict = {word.lower(): 1 for word in filtered_positive_words}
    negative_word_dict = {word.lower(): -1 for word in filtered_negative_words}

    # Sentiment Analysis using the positive and negative word dictionaries
    positive_score = sum(positive_word_dict.get(word.lower(), 0) for word in blob.words)
    negative_score = sum(negative_word_dict.get(word.lower(), 0) for word in blob.words)
    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)
    cleaned_tokens = [word for word in blob.words if word.lower() not in stop_words]
    total_words = len(cleaned_tokens)

    # Calculate Subjectivity Score
    subjectivity_score = (positive_score + negative_score) / (total_words + 1e-6)

    complex_word_count = sum(1 for word in cleaned_tokens if len(word) > 2 and word.lower() not in words.words())
    complex_word_percentage = (complex_word_count / word_count) * 100

    # Calculate Word Count (excluding punctuations)
    word_count = len([word for word in cleaned_tokens if word.isalpha()])
    total_characters = sum(len(word) for word in cleaned_tokens)
    average_word_length = total_characters / total_words

    # Calculate Syllable Count Per Word
    def count_syllables(word):
        # Count the number of vowels in the word
        vowels = 'AEIOUaeiou'
        syllables = 0
        prev_char = None
        for char in word:
            if char in vowels and (prev_char is None or prev_char not in vowels):
              syllables += 1
            prev_char = char
        # Handle special cases
        if word.endswith('es') or word.endswith('ed'):
            syllables -= 1
        if word.endswith('le') and len(word) > 2:
            syllables += 1
        return max(1, syllables)  # At least one syllable

    syllable_count = sum(count_syllables(word) for word in cleaned_tokens)


    # Calculate Personal Pronoun Count
    personal_pronouns = set(['I', 'we', 'my', 'ours', 'us'])
    personal_pronoun_count = sum(1 for word in cleaned_tokens if word.lower() in personal_pronouns)

    # Sentence Analysis
    sentences = sent_tokenize(' '.join(cleaned_tokens))  # Use cleaned text
    total_sentences = len(sentences)
    average_sentence_length = total_words / total_sentences
    total_words_per_sentence = [len(word_tokenize(sentence)) for sentence in sentences]
    avg_words_per_sentence = sum(total_words_per_sentence) / len(sentences)
    total_complex_words = sum(1 for word in cleaned_tokens if len(word) > 2 and word.lower() not in words.words())
    percentage_complex_words = (total_complex_words / total_words) * 100
    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)


    result = [url_id,url, positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\
                            complex_word_percentage, fog_index, avg_words_per_sentence, complex_word_count,\
                            word_count, syllable_count, personal_pronoun_count, avg_word_length]
    return result

output_excel_path = '/content/drive/MyDrive/20211030 Test Assignment-20230808T121525Z-001/20211030 Test Assignment/Output_Data_Structure.xlsx'
output_df = pd.read_excel(output_excel_path)

# Load the Excel file
excel_file_path = '/content/drive/MyDrive/20211030 Test Assignment-20230808T121525Z-001/20211030 Test Assignment/Input.xlsx'
df = pd.read_excel(excel_file_path)

url_column = df.iloc[:, 1][0:]
Url_id_column = df.iloc[:, 0][1:]

# Get the URLs and URL IDs from the dataframe
url_column = df['URL']
url_id_column = df['URL_ID']

# Create the output directory if it doesn't exist
output_directory = "/content/drive/MyDrive/output_files"
os.makedirs(output_directory, exist_ok=True)

def extract_article_content(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find the article content using one of the two class names
        article_content = soup.select_one('.td-post-content.tagdiv-type') or soup.select_one('.tdb_single_content.tdb-block-inner.td-fix-index')

        if article_content:
          # Find the title element within the article content
          title_element = soup.find('h1', class_='entry-title') or soup.find('h1', class_='tdb-title-text')

          if title_element:
            article_title = title_element.get_text().strip()
            article_text = ' '.join([p.get_text() for p in article_content.find_all('p')])
            return article_title, article_text
        else:
            return None, None

    except Exception as e:
        print(f"Error extracting content from {url}: {e}")
        return None, None

# Iterate through each URL and URL ID
for i, (url, url_id) in enumerate(zip(url_column, url_id_column)):
    if pd.notnull(url):  # Check for valid URLs, excluding 'nan' values
        article_title, article_text = extract_article_content(url)

        if article_title and article_text:
            # Construct the file path using URL ID
            text_file_path = os.path.join(output_directory, f"{url_id}.txt")

            # Create and write to the text file
            with open(text_file_path, 'w', encoding='utf-8') as text_file:
                text_file.write(f"Title: {article_title}\n\n")
                text_file.write(article_text)
            print(f"Saved content from {url} to {text_file_path}")
            Article_Analysis = perform_text_analysis(url, url_id, article_text)
            output_df.iloc[i,:] = Article_Analysis
            output_df.to_excel(output_excel_path, index=False)
        else:
            print(f"Skipped invalid content from {url}")
            for col_index in range(2, 15):  # Columns from 3rd to 13th
                output_df.iloc[i, col_index] = "Null"
            # Save the updated output_datastructure.xlsx
            output_df.to_excel(output_excel_path, index=False)

    else:
        print(f"Skipping invalid URL at index {i}: {url}")

file_list = os.listdir(output_directory)

# Count the number of files
number_of_files = len(file_list)

print(f"Number of files in the directory: {number_of_files}")

structure_file_path = '/content/drive/MyDrive/20211030 Test Assignment-20230808T121525Z-001/20211030 Test Assignment/Output_Data_Structure.xlsx'
structure_df = pd.read_excel(structure_file_path)

from textblob import TextBlob
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import words
import pandas as pd

!pip install nltk

import nltk
nltk.download('punkt')

nltk.download('words')

import chardet

# Load the master dictionary files
master_dict_folder = '/content/drive/MyDrive/20211030 Test Assignment-20230808T121525Z-001/20211030 Test Assignment/MasterDictionary'
positive_words_path = os.path.join(master_dict_folder, 'positive-words.txt')
negative_words_path = os.path.join(master_dict_folder, 'negative-words.txt')

def detect_encoding(file_path):
    with open(file_path, 'rb') as file:
        result = chardet.detect(file.read())
        return result['encoding']

encoding = detect_encoding(negative_words_path)
print("Detected Encoding:", encoding)

def load_word_list(file_path, encoding='utf-8'):
  with open(file_path, 'r', encoding=encoding) as f:
    return set([word.strip() for word in f.readlines()])

positive_words = load_word_list(positive_words_path)
negative_words = load_word_list(negative_words_path, encoding = 'ISO-8859-1')

stop_words_directory = '/content/drive/MyDrive/20211030 Test Assignment-20230808T121525Z-001/20211030 Test Assignment/StopWords'

def load_all_stop_words(directory_path):
    all_stop_words = set()

    for filename in os.listdir(directory_path):
        if filename.endswith(".txt"):
            file_path = os.path.join(directory_path, filename)
            with open(file_path, 'r', encoding='latin-1') as file:
                stop_words_in_file = set([word.strip() for word in file.readlines()])
                all_stop_words.update(stop_words_in_file)

    return all_stop_words

stop_words = load_all_stop_words(stop_words_directory)

# Load the Output Data Structure Excel file
structure_file_path = '/content/drive/MyDrive/20211030 Test Assignment-20230808T121525Z-001/20211030 Test Assignment/Output_Data_Structure.xlsx'
structure_df = pd.read_excel(structure_file_path)

def perform_text_analysis(text):
    blob = TextBlob(text)
    positive_words = set(...)  # Set of positive words
    negative_words = set(...)  # Set of negative words

    # Sentiment Analysis using master dictionary
    positive_score = sum(1 for word in blob.words if word.lower() in positive_words)
    negative_score = sum(1 for word in blob.words if word.lower() in negative_words)
    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 1e-10)
    subjectivity_score = blob.sentiment.subjectivity

    # Other variable calculations
    personal_pronouns = set(['I', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them', 'their', 'theirs'])
    # Personal Pronoun Analysis
    personal_pronoun_count = sum(1 for word in blob.words if word.lower() in personal_pronouns)

    # Sentence Analysis
    sentences = sent_tokenize(text)
    avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)
    avg_words_per_sentence = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sent_tokenize(text))

    # Word Analysis
    word_count = len(word_tokenize(text))
    syllable_count = sum(len(word) for word in word_tokenize(text)) - text.count(' ')
    complex_word_count = sum(1 for word in word_tokenize(text) if len(word) > 2 and word.lower() not in words.words())
    complex_word_percentage = (complex_word_count / word_count) * 100
    fog_index = 0.4 * (avg_words_per_sentence + complex_word_percentage)
    avg_word_length = syllable_count / word_count

    return positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\
                            complex_word_percentage, fog_index, avg_words_per_sentence, complex_word_count,\
                            word_count, syllable_count, personal_pronoun_count, avg_word_length

# Initialize a list to store results
results = []

# Iterate through each row in the structure DataFrame
for index, row in structure_df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Construct the filename based on URL_ID
    filename = f"{url.replace('/', '_')}.txt"
    file_path = os.path.join(output_directory, filename)
    print(file_path)

    # Read the content of the text file
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()

    # Perform text analysis
    blob = TextBlob(text)

    # Sentiment Analysis using master dictionary
    positive_score = sum(1 for word in blob.words if word.lower() in positive_words)
    negative_score = sum(1 for word in blob.words if word.lower() in negative_words)
    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 1e-10)
    subjectivity_score = blob.sentiment.subjectivity


    # Other variable calculations
    # Sentence Analysis
    sentences = sent_tokenize(text)
    avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)
    avg_words_per_sentence = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sent_tokenize(text))

    # Word Analysis
    word_count = len(word_tokenize(text))
    syllable_count = sum(len(word) for word in word_tokenize(text)) - text.count(' ')
    complex_word_count = sum(1 for word in word_tokenize(text) if len(word) > 2 and word.lower() not in words.words())
    complex_word_percentage = (complex_word_count / word_count) * 100
    fog_index = 0.4 * (avg_words_per_sentence + complex_word_percentage)
    avg_word_length = syllable_count / word_count


    # Append variables to the results list
    result_row = [positive_score, negative_score, polarity_score, subjectivity_score,
                  avg_sentence_length, complex_word_percentage,
                  fog_index, avg_words_per_sentence, complex_word_count,
                  word_count, syllable_count / word_count, blob.words.count('I'), avg_word_length]

    results.append(result_row)

# Add the calculated variables to the structure DataFrame
variable_columns = structure_df.columns[2:].tolist()
for col_index, col_name in enumerate(variable_columns):
    structure_df[col_name] = [row[col_index] for row in results]

# Save the DataFrame to the output Excel file
output_excel_path = '/content/drive/MyDrive/path/to/output/results_with_variables.xlsx'
structure_df.to_excel(output_excel_path, index=False)

print("Textual analysis and variable computation complete.")